{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWk1vO-Rf0As"
      },
      "source": [
        "<h1>Classification</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHPmbZVQf0Av"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from timeit import default_timer as timer\n",
        "import emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, json\n",
        "from sklearn.utils import shuffle\n",
        "import string\n",
        "import html\n",
        "import re\n",
        "import pickle\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, classification_report\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QXnUbMDf0Ay"
      },
      "source": [
        "Come primo step, vado a scaricare i datasets necessari (tramite il comando *wget*), che contengono tweet classificati con 5 differenti tipi di emozioni:\n",
        "* Anger (rabbia)\n",
        "    - Training set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/anger-ratings-0to1.train.txt\n",
        "    - Test set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/anger-ratings-0to1.test.target.txt\n",
        "* Fear (paura)\n",
        "    - Training set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/fear-ratings-0to1.train.txt\n",
        "    - Test set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/fear-ratings-0to1.test.target.txt\n",
        "* Joy (gioia)\n",
        "    - Training set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/joy-ratings-0to1.train.txt\n",
        "    - Test set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/joy-ratings-0to1.test.target.txt\n",
        "* Sadness (tristezza)\n",
        "    - Training set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/sadness-ratings-0to1.train.txt\n",
        "    - Test set $\\rightarrow$ http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/sadness-ratings-0to1.test.target.txt\n",
        "\n",
        "\n",
        "Ogni file viene rinominato con il rispettivo sentiment.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysKncXFEf0A2",
        "outputId": "2a661af8-55ed-4ab6-919d-c984fde237f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2021-06-09 16:22:24--  http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/anger-ratings-0to1.train.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 95686 (93K) [text/plain]\n",
            "Saving to: â€˜anger.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 53%  118K 0s\n",
            "    50K .......... .......... .......... .......... ...       100%  284K=0.6s\n",
            "\n",
            "2021-06-09 16:22:25 (162 KB/s) - â€˜anger.csvâ€™ saved [95686/95686]\n",
            "\n",
            "--2021-06-09 16:22:26--  http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/fear-ratings-0to1.train.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 133186 (130K) [text/plain]\n",
            "Saving to: â€˜fear.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 38%  108K 1s\n",
            "    50K .......... .......... .......... .......... .......... 76%  218K 0s\n",
            "   100K .......... .......... ..........                      100%  329K=0.8s\n",
            "\n",
            "2021-06-09 16:22:27 (166 KB/s) - â€˜fear.csvâ€™ saved [133186/133186]\n",
            "\n",
            "--2021-06-09 16:22:27--  http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/joy-ratings-0to1.train.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92266 (90K) [text/plain]\n",
            "Saving to: â€˜joy.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 55%  124K 0s\n",
            "    50K .......... .......... .......... ..........           100%  238K=0.6s\n",
            "\n",
            "2021-06-09 16:22:28 (158 KB/s) - â€˜joy.csvâ€™ saved [92266/92266]\n",
            "\n",
            "--2021-06-09 16:22:28--  http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/sadness-ratings-0to1.train.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92977 (91K) [text/plain]\n",
            "Saving to: â€˜sadness.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 55%  126K 0s\n",
            "    50K .......... .......... .......... ..........           100%  239K=0.6s\n",
            "\n",
            "2021-06-09 16:22:29 (160 KB/s) - â€˜sadness.csvâ€™ saved [92977/92977]\n",
            "\n",
            "--2021-06-09 16:22:29--  http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/anger-ratings-0to1.test.target.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86506 (84K) [text/plain]\n",
            "Saving to: â€˜anger2.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 59%  119K 0s\n",
            "    50K .......... .......... .......... ....                 100%  212K=0.6s\n",
            "\n",
            "2021-06-09 16:22:31 (145 KB/s) - â€˜anger2.csvâ€™ saved [86506/86506]\n",
            "\n",
            "--2021-06-09 16:22:31--  http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/fear-ratings-0to1.test.target.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113260 (111K) [text/plain]\n",
            "Saving to: â€˜fear2.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 45%  128K 0s\n",
            "    50K .......... .......... .......... .......... .......... 90%  195K 0s\n",
            "   100K ..........                                            100%  451K=0.7s\n",
            "\n",
            "2021-06-09 16:22:32 (165 KB/s) - â€˜fear2.csvâ€™ saved [113260/113260]\n",
            "\n",
            "--2021-06-09 16:22:32--  http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/joy-ratings-0to1.test.target.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79311 (77K) [text/plain]\n",
            "Saving to: â€˜joy2.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 64%  126K 0s\n",
            "    50K .......... .......... .......                         100%  188K=0.5s\n",
            "\n",
            "2021-06-09 16:22:33 (143 KB/s) - â€˜joy2.csvâ€™ saved [79311/79311]\n",
            "\n",
            "--2021-06-09 16:22:33--  http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/sadness-ratings-0to1.test.target.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78233 (76K) [text/plain]\n",
            "Saving to: â€˜sadness2.csvâ€™\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 65%  101K 0s\n",
            "    50K .......... .......... ......                          100%  501K=0.5s\n",
            "\n",
            "2021-06-09 16:22:35 (139 KB/s) - â€˜sadness2.csvâ€™ saved [78233/78233]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "wget -O anger.csv http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/anger-ratings-0to1.train.txt\n",
        "wget -O fear.csv http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/fear-ratings-0to1.train.txt\n",
        "wget -O joy.csv http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/joy-ratings-0to1.train.txt\n",
        "wget -O sadness.csv http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/sadness-ratings-0to1.train.txt\n",
        "\n",
        "\n",
        "wget -O anger2.csv http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/anger-ratings-0to1.test.target.txt\n",
        "wget -O fear2.csv http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/fear-ratings-0to1.test.target.txt\n",
        "wget -O joy2.csv http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/joy-ratings-0to1.test.target.txt\n",
        "wget -O sadness2.csv http://saifmohammad.com/WebDocs/EmoInt%20Test%20Data/sadness-ratings-0to1.test.target.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzOljZ7ef0A5"
      },
      "source": [
        "Oltre ad utilizzare i 4 dataset precedenti, vado ad utilizzare anche il dataset \"*text_emotion*\" presente in __[questo link](https://data.world/crowdflower/sentiment-analysis-in-text#)__. <br>\n",
        "Vado quindi a leggere i datasets con l'opportuno carattere separatore ed *encoding*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce5x8NbPf0A7"
      },
      "outputs": [],
      "source": [
        "anger1 = pd.read_csv('anger.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "fear1 = pd.read_csv('fear.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "joy1 = pd.read_csv('joy.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "sadness1 = pd.read_csv('sadness.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "\n",
        "\n",
        "anger2 = pd.read_csv('anger2.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "fear2 = pd.read_csv('fear2.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "joy2 = pd.read_csv('joy2.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "sadness2 = pd.read_csv('sadness2.csv',sep='\\t',header=None,encoding='utf-8')\n",
        "\n",
        "# https://data.world/crowdflower/sentiment-analysis-in-text#\n",
        "text_emotion = pd.read_csv('text_emotion.csv',encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ImGbvBgf0A9"
      },
      "source": [
        "Anche se i dataset sono già suddivisi in *training set* e *test set*, per rendere il tutto più uniforme, vado ad combinarli, in modo da ottenere un dataset unico per ciascun sentiment: questa operazione viene effettuata tramite la funzione *concat* di Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0fEzK_7f0A-"
      },
      "outputs": [],
      "source": [
        "anger = pd.concat([anger1,anger2])\n",
        "fear = pd.concat([fear1,fear2])\n",
        "joy = pd.concat([joy1,joy2])\n",
        "sadness = pd.concat([sadness1,sadness2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tidph7xf0BA"
      },
      "source": [
        "A partire dai 4 dataset, tramite *concat* li unisco, rinomino le colonne e seleziono solamente quelle che mi servono, ovvero il tweet vero e proprio e il sentiment; dopodichè effettuo un mapping sentiment-intero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3E6QxHCf0BB"
      },
      "outputs": [],
      "source": [
        "all_emo = pd.concat([anger,fear,joy,sadness])\n",
        "all_emo.columns = ['id','tweet','sentiment','score']\n",
        "all_emo = all_emo[['tweet','sentiment']]\n",
        "\n",
        "all_emo.sentiment.replace({\n",
        "    'anger' : 0,\n",
        "    'fear' : 1,\n",
        "    'joy' : 2,\n",
        "    'sadness' : 3,\n",
        "},inplace=True)\n",
        "\n",
        "\n",
        "all_emo = all_emo.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmgAazr6f0BD"
      },
      "source": [
        "Per bilanciare i vari sentiment, una scelta opportuna è quella di avere a disposizione anche un sentiment di tipo *neutrale*: vado quindi ad estrarre da *text_emotion* tutti quei tweet che hanno sentiment vuoto o neutrale. <br>\n",
        "Per bilanciare il numero di tweet per le varie classi, vado come prima cosa a verificare il numero di tweet disponibili per ogni sentiment e ne calcolo la media:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1flcAq4uf0BE",
        "outputId": "8dea33a8-ca82-4361-c8b0-451cad4953bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1617, 4) (2142, 4) (1537, 4) (1459, 4)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1688"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(anger.shape, fear.shape, joy.shape, sadness.shape)\n",
        "\n",
        "res = (anger.shape[0] + fear.shape[0] + joy.shape[0] + sadness.shape[0]) / 4\n",
        "int(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbWAC-Eof0BF"
      },
      "source": [
        "In questo caso, le classi non risultano particolarmente sbilanciate. <br> <br>\n",
        "L'idea è quindi quella di prendere dal DataFrame *text_emotion* 1688 tweets con sentiment neutrale o vuoto. <br>\n",
        "Tuttavia, se vado a selezionare 1688 / 2 = 844 tweet con sentiment vuoto e 844 tweet con sentiment neutrale, la funzione *sample* presente in seguito restituisce errore, in quanto nel DataFrame *text_emotion* non è presente un numero sufficiente di tweets con sentiment vuoto o neutrale: per risolvere questa problematica, considero 1500 tweets anziché 1688, in modo da prelevare quindi 750 tweets con sentiment vuoto e 750 tweets con sentiment neutrale. <br> <br>\n",
        "Oltre questo, vado a prelevare e rinominare le colonne che mi interessano e vado ad effettuare il mapping sentiment-intero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNZfjFPFf0BG"
      },
      "outputs": [],
      "source": [
        "text_emotion = text_emotion[['content','sentiment']]\n",
        "text_emotion.columns = ['tweet','sentiment']\n",
        "empty = text_emotion[text_emotion['sentiment'] == 'empty'].sample(n = 750)\n",
        "neutral = text_emotion[text_emotion['sentiment'] == 'neutral'].sample(n = 750)\n",
        "text_emotion = pd.concat([empty, neutral])\n",
        "text_emotion.sentiment.replace({\n",
        "    'empty' : 4,\n",
        "    'neutral' : 4,\n",
        "},inplace = True)\n",
        "\n",
        "text_emotion = text_emotion.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X74hjjo8f0BG"
      },
      "source": [
        "A questo punto, metto insieme i vari *DataFrame* creati in precedenza in uno unico, chiamato ***df***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlSm_6d3f0BH",
        "outputId": "5a830c4e-871d-43d4-c43e-020b423b5f33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So my Indian Uber driver just called someone t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>*sigh* joe sings so purdy.  he makes me feel b...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>Disneyland was a blast yesterday now back to work</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>GOING TO WATCH SUPERNATURAL shall return after...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>(@ohmyjade) 1-Pendulum = awesome! 2-Goodbyes s...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>- doesn`t have a phone.</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8255 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweet  sentiment\n",
              "0     How the fu*k! Who the heck! moved my fridge!.....          0\n",
              "1     So my Indian Uber driver just called someone t...          0\n",
              "2     @DPD_UK I asked for my parcel to be delivered ...          0\n",
              "3     so ef whichever butt wipe pulled the fire alar...          0\n",
              "4     Don't join @BTCare they put the phone down on ...          0\n",
              "...                                                 ...        ...\n",
              "1495  *sigh* joe sings so purdy.  he makes me feel b...          4\n",
              "1496  Disneyland was a blast yesterday now back to work          4\n",
              "1497  GOING TO WATCH SUPERNATURAL shall return after...          4\n",
              "1498  (@ohmyjade) 1-Pendulum = awesome! 2-Goodbyes s...          4\n",
              "1499                            - doesn`t have a phone.          4\n",
              "\n",
              "[8255 rows x 2 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.concat([all_emo, text_emotion])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2KAlISef0BJ"
      },
      "source": [
        "Come ulteriore check, controllo quante classi sono *uniche*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h1zg2Iqf0BK",
        "outputId": "2c6d3c74-c952-47bc-d8bc-d077c0583748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4], dtype=int64)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sentiment.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQTrG6Zgf0BL"
      },
      "source": [
        "E verifico se sono presenti valori *null*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecA2TQHif0BL",
        "outputId": "7760b7e9-f171-4c0d-81d8-6f0b3384583d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tweet        0.0\n",
              "sentiment    0.0\n",
              "dtype: float64"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(df.isnull().sum() / len(df))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c3B4krEf0BM"
      },
      "source": [
        "Salvo il dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEHkOwozf0BM"
      },
      "outputs": [],
      "source": [
        "df.to_csv('dataset.csv',encoding='utf-8',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4t-uY1Ef0BN"
      },
      "source": [
        "Come ulteriore check, stampo il numero di tweet a disposizione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZsP3o19f0BO",
        "outputId": "68308c74-2c46-4ebf-c2df-93b7a4ed42f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total length of the data: 8255\n",
            "# of anger tagged sentences: 1617\n",
            "# of fear tagged sentences: 2142\n",
            "# of joy tagged sentences: 1537\n",
            "# of sadness tagged sentences: 1459\n",
            "# of neutral tagged sentences: 1500\n"
          ]
        }
      ],
      "source": [
        "anger_sentences = df['sentiment'][df.sentiment == 0]\n",
        "fear_sentences = df['sentiment'][df.sentiment == 1]\n",
        "joy_sentences = df['sentiment'][df.sentiment == 2]\n",
        "sadness_sentences = df['sentiment'][df.sentiment == 3]\n",
        "neutral_sentences = df['sentiment'][df.sentiment == 4]\n",
        "\n",
        "\n",
        "print(f\"Total length of the data: {df.shape[0]}\")\n",
        "print(f\"# of anger tagged sentences: {len(anger_sentences)}\")\n",
        "print(f\"# of fear tagged sentences: {len(fear_sentences)}\")\n",
        "print(f\"# of joy tagged sentences: {len(joy_sentences)}\")\n",
        "print(f\"# of sadness tagged sentences: {len(sadness_sentences)}\")\n",
        "print(f\"# of neutral tagged sentences: {len(neutral_sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ull2l02f0BO"
      },
      "source": [
        "Converto la colonna \"*tweet*\" del DataFrame in stringa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-J7ZPHSf0BP"
      },
      "outputs": [],
      "source": [
        "df['tweet'] = df['tweet'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXYwSo8Gf0BP"
      },
      "source": [
        "<h1>Pre Processing</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKvGKKff0BP"
      },
      "source": [
        "Cercando in rete, ho trovato questo dizionario contenente le forme contratte e le rispettive forme regolari: questo dizionario sarà necessario in seguito.\n",
        "* __[Link Medium](https://medium.com/coinmonks/remaking-of-shortened-sms-tweet-post-slangs-and-word-contraction-into-sentences-nlp-7bd1bbc6fcff)__ \n",
        "* __[Link Github](https://gist.github.com/neelindresh/39b3b4c3113d30a6e796697ff9e5fc12#file-contractedtext-py)__ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwcTiLRVf0BP"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/coinmonks/remaking-of-shortened-sms-tweet-post-slangs-and-word-contraction-into-sentences-nlp-7bd1bbc6fcff\n",
        "# https://gist.github.com/neelindresh/39b3b4c3113d30a6e796697ff9e5fc12#file-contractedtext-py\n",
        "\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\", \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDZyOa_Vf0BQ"
      },
      "source": [
        "Per \"cercare di correggere\" gli errori nelle recensioni, a partire dal dizionario con le contrazioni, creo un secondo dizionario, avente come chiave la chiave del dizionario delle contrazioni <b>senza</b> apostrofo: successivamente, tramite l'operatore \\*\\*, unisco i due dizionari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoSEwRApf0BR"
      },
      "outputs": [],
      "source": [
        "CONTRACTION_MAP_WO_QUOTE = dict(zip([i.replace(\"'\",\"\") for i in CONTRACTION_MAP.keys()], CONTRACTION_MAP.values()))\n",
        "CONTRACTION_MAP = {**CONTRACTION_MAP, **CONTRACTION_MAP_WO_QUOTE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDUlI_r_f0BR"
      },
      "source": [
        "Per cercare di rendere le recensioni \"il più pulite\" possibili, vado ad espandere le possibili abbreviazioni, come ad esempio: <br>\n",
        "asap $\\rightarrow$ as son as possible <br>\n",
        "<br>\n",
        "Sempre cercando in rete, sono riuscito a trovare questa pagina chiamata <i>acronymslist</i> che contiene degli acronimi e i suoi rispettivi significati. Tra tutti, vado a prendere solamente quelli che si usano tipicamente per chattare o in generale nelle conversazioni poco formali. <br>\n",
        "Per estrarli, vado ad utilizzare la libreria <i>BeautifulSoup</i>, visto che si trattano di pagine HTML statiche che non utilizzano JavaScript. <br> <br>\n",
        "\n",
        "Per ogni url, vado ad estrarre gli acronimi e i loro significati, entrambi inseriti in delle liste e successivamente unite insieme per creare un dizionario chiamato <i>ACRONYMS_MAP</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5N15pSnf0BS"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://www.acronymslist.com/cat/chatting-acronyms.html\",\n",
        "    \"https://www.acronymslist.com/cat/chatting-acronyms-p1.html\",\n",
        "    \"https://www.acronymslist.com/cat/chatting-acronyms-p2.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p2.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p3.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p4.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p5.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p6.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p7.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p8.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p9.html\",\n",
        "    \"https://www.acronymslist.com/cat/sms-chat-and-text-acronyms-p10.html\",\n",
        "]\n",
        "\n",
        "acronyms = []\n",
        "meanings = []\n",
        "ACRONYMS_MAP = {}\n",
        "\n",
        "for url in urls:\n",
        "    req = requests.get(url)\n",
        "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
        "    a = soup.find_all(\"a\", {\"class\": \"special\"})\n",
        "    td = soup.find_all('td', {'width' : '450','style':'vertical-align: middle'})\n",
        "    for i in a:\n",
        "        acronyms.append(i.text.lower())\n",
        "    for i in td:\n",
        "        meanings.append(i.text.lower())\n",
        "\n",
        "ACRONYMS_MAP = dict(zip(acronyms,meanings))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reuF-Kv-f0BS"
      },
      "source": [
        "Per evitare la sostituzione di parole necessarie, vado a verificare se l'acronimo è una parola appartenente al dizionario della lingua inglese: se ciò accade, allora lo vado ad eliminare dal dizionario. <br>\n",
        "Ad esempio, il termine <i>so</i> sta per <i>significant other</i>: in questo modo, non trasformo <i>so</i> in <i>significant other</i> in modo da poter avere una sentiment \"più precisa\" (ad esempio nelle parole del tipo \"so good\" ecc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ODR9e9If0BS"
      },
      "outputs": [],
      "source": [
        "setofwords = set(words.words())\n",
        "\n",
        "for key in list(ACRONYMS_MAP.keys()):\n",
        "    if key in setofwords:\n",
        "        del ACRONYMS_MAP[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsbO-bv2f0BT"
      },
      "source": [
        "A questo punto, a partire dai due dizionari, ovvero quello delle contrazioni e quello degli acronimi, ne vado a creare un singolo, fondendoli insieme sempre tramite l'operatore \\*\\*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL07Xg12f0BT",
        "outputId": "82e8d716-b3d7-48c6-d842-9b6329e9b829"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'afaik': 'as far as i know',\n",
              " 'afk': 'away from keyboard',\n",
              " 'asap': 'as soon as possible',\n",
              " 'b4': 'before',\n",
              " 'bak': 'back at keyboard',\n",
              " 'bbl': 'be back later',\n",
              " 'bbs': 'be back soon',\n",
              " 'bcnu': 'be seeing you',\n",
              " 'bfn': 'bye for now',\n",
              " 'brb': 'be right back',\n",
              " 'btdt': 'been there, done that',\n",
              " 'btw': 'by the way',\n",
              " 'byoh': 'bat you on a head',\n",
              " 'cfv': 'call for votes',\n",
              " 'cu': 'see you',\n",
              " 'cul': 'see you later',\n",
              " 'cul8r': 'see you later',\n",
              " 'dyjhiw': \"don't you just hate it when...\",\n",
              " 'eg': 'evil grin',\n",
              " 'etla': 'extended three letter acronym',\n",
              " 'f2f': 'face to face (also meeting in person)',\n",
              " 'faq': 'frequently asked questions',\n",
              " 'ffs': 'for f*cks sake',\n",
              " 'foad': 'f*ck off and die',\n",
              " 'fubar': 'f*cked up beyond all repair / recognition',\n",
              " 'fwiw': \"for what it's worth\",\n",
              " 'fya': 'for your amusement',\n",
              " 'fyi': 'for your information',\n",
              " 'gbh': 'great big hug',\n",
              " 'gbh&k': 'great back hug & kiss',\n",
              " 'gr&d': 'grinning, running & ducking',\n",
              " 'hb': 'hug back',\n",
              " 'hh': 'holding hands',\n",
              " 'hhoj/k': 'ha ha, only joking/kidding',\n",
              " 'hhos': 'ha ha, only serious',\n",
              " 'hiwth': 'hate it when that happens',\n",
              " 'iae': 'in any event',\n",
              " 'ianal': 'i am not a lawyer',\n",
              " 'ibn': \"i'm butt naked!\",\n",
              " 'idk': \"i don't know\",\n",
              " 'imho': 'in my humble opinion',\n",
              " 'imnsho': 'in my not so humble opinion',\n",
              " 'imo': 'in my opinion',\n",
              " 'iow': 'in other words',\n",
              " 'irl': 'in real life',\n",
              " 'iykwim': 'if you know what i mean',\n",
              " 'j/k': 'just kidding',\n",
              " 'jase': 'just another system error',\n",
              " 'kb': 'kiss back',\n",
              " 'kotc': 'kiss on the cheek',\n",
              " 'l8r': 'later',\n",
              " 'lmao': 'laughing my a** off',\n",
              " 'lol': 'laugh out loud',\n",
              " 'lylab/s': 'love you like a brother/sister',\n",
              " 'morf': 'male or female',\n",
              " 'motas': 'members of the appropriate sex',\n",
              " 'motos': 'member of the opposite sex',\n",
              " 'motss': 'members of the same sex',\n",
              " 'nfw': 'no f*cking way',\n",
              " 'nifoc': 'naked in front of computer',\n",
              " 'nrn': 'no response / reply necessary',\n",
              " 'obtw': 'oh, by the way',\n",
              " 'oic': 'oh i see',\n",
              " 'otoh': 'on the other hand',\n",
              " 'ott': 'over the top',\n",
              " 'pd': 'public domain',\n",
              " 'pmfji': 'pardon me for jumping in',\n",
              " 'pmji': ' pardon me',\n",
              " 'potc': 'peck on the cheek',\n",
              " 'rehi': 'hello again',\n",
              " 'rfd': 'request for discussion',\n",
              " 'rotfl': 'rolling on the floor laughing',\n",
              " 'rotflmao': 'rolling on the floor laughing my ass off',\n",
              " 'rsn': 'real soon now',\n",
              " 'rtfm': 'read the f*cking manual',\n",
              " 'ruok': 'are you ok?',\n",
              " 'sb': 'smiles back',\n",
              " 'sitd': 'still in the dark',\n",
              " 'stfu': 'shut the f*ck up',\n",
              " 'sw': 'shareware',\n",
              " 'syl': 'see you later',\n",
              " 'tanstaafl': \"there ain't no such thing as a free lunch\",\n",
              " 'tarfu': 'things are really f*cked up',\n",
              " 'tgif': \"thank god it's friday\",\n",
              " 'tia': 'thanks in advance',\n",
              " 'tla': 'three-letter acronym',\n",
              " '2l8': 'too late',\n",
              " '2u2': 'to you too',\n",
              " '404': \"i haven't a clue - from the internet 404 page not found error.\",\n",
              " '4gm': 'forgive me',\n",
              " '::poof::': 'goodbye (leaving the room)',\n",
              " '>u!': 'screw you!',\n",
              " '?': 'huh?',\n",
              " '?4u': 'question for you',\n",
              " 'a/s/l?': 'age/sex/location?',\n",
              " 'aamof': 'as a matter of fact',\n",
              " 'aayf': 'as always, your friend',\n",
              " 'abt2': 'about to',\n",
              " 'adn': 'any day now',\n",
              " 'afaic': \"as far as i'm concerned\",\n",
              " 'alol': 'actually laughing out loud',\n",
              " 'aml': 'all my love',\n",
              " 'asl': 'age, sex, location?',\n",
              " 'aslmh': 'age, sex, location, music, hobbies?',\n",
              " 'aslp': 'age, sex, location, picture?',\n",
              " 'atm': 'at the moment',\n",
              " 'atst': 'at the same time',\n",
              " 'atys': 'anything you say',\n",
              " 'awol': 'absent without leave',\n",
              " 'ayk': 'as you know',\n",
              " 'aysos': 'are you stupid or something',\n",
              " 'aytmtb': \"and you're telling me this because?\",\n",
              " 'b/c': 'because',\n",
              " 'b4n': 'bye for now',\n",
              " 'bbiab': 'be back in a bit',\n",
              " 'bbn': 'bye bye now',\n",
              " 'bbt': 'be back tomorrow',\n",
              " 'bc': 'be cool',\n",
              " 'bd': 'big deal',\n",
              " 'bf': 'boy friend',\n",
              " 'bfd': 'big f***ing deal',\n",
              " 'bg': 'big grin',\n",
              " 'bibo': 'beer in, beer out',\n",
              " 'bil': 'brother in law',\n",
              " 'bioyiop': 'blow it out your i/o port',\n",
              " 'bl': 'belly laughing',\n",
              " 'bme': 'based on my experience',\n",
              " 'bmgwl': 'busting my gut with laughter',\n",
              " 'botec': 'back-of-the-envelope calculation',\n",
              " 'brh': 'be right here',\n",
              " 'bta': 'but then again...',\n",
              " 'btsoom': 'beats the s*** out of me',\n",
              " 'bw': 'best wishes',\n",
              " 'bwl': 'bursting with laughter',\n",
              " 'bwthdik': 'but what the heck do i know...?',\n",
              " 'bykt': 'but you knew that',\n",
              " 'bytm': 'better you than me',\n",
              " 'c&g': 'chuckle and grin',\n",
              " 'cico': 'coffee in, coffee out',\n",
              " 'cid': 'consider it done',\n",
              " 'cmiiw': \"correct me if i'm wrong\",\n",
              " 'cnp': 'continued in next post',\n",
              " 'crb': 'come right back',\n",
              " 'crbt': 'crying real big tears',\n",
              " 'csl': \"can't stop laughing\",\n",
              " 'cus': 'see you soon',\n",
              " 'cwot': 'complete waste of time',\n",
              " 'cya': 'cover your ass',\n",
              " 'cyl': 'see you later',\n",
              " 'cyo': 'see you online',\n",
              " 'cyt': 'see you tomorrow',\n",
              " 'dba': 'doing business as',\n",
              " 'dga': \"don't go anywhere\",\n",
              " 'diik': 'darned if i know',\n",
              " 'diku': 'do i know you?',\n",
              " 'dityid': \"did i tell you i'm distressed?\",\n",
              " 'diy': 'do it yourself',\n",
              " 'dl': 'dead link',\n",
              " 'dltbbb': \"don't let the bed bugs bite\",\n",
              " 'dltm': \"don't lie to me\",\n",
              " 'dqmot': \"don't quote me on this\",\n",
              " 'dtrt': 'do the right thing',\n",
              " 'dwb': \"don't write back\",\n",
              " 'eak': 'eating at keyboard',\n",
              " 'emfbi': 'excuse me for butting in',\n",
              " 'emsg': 'e-mail message',\n",
              " 'eod': 'end of discussion',\n",
              " 'eol': 'end of lecture',\n",
              " 'eom': 'end of message',\n",
              " 'eor': 'end of rant',\n",
              " 'eot': 'end of thread (meaning: end of discussion)',\n",
              " 'fc': 'fingers crossed',\n",
              " 'fcol': 'for crying out loud',\n",
              " 'ff': 'friends forever',\n",
              " 'fil': 'father in law',\n",
              " 'fitb': 'fill in the blanks',\n",
              " 'fla': 'four-letter acronym',\n",
              " 'fmtyewtk': 'far more than you ever wanted to know',\n",
              " 'fnb': 'football and beer',\n",
              " 'focl': 'falling off the chair laughing',\n",
              " 'fofl': 'falling on the floor laughing',\n",
              " 'fomcl': 'falling off my chair laughing',\n",
              " 'ftbomh': 'from the bottom of my heart',\n",
              " 'fym': 'for your misinformation',\n",
              " 'g2g': 'gotta go',\n",
              " 'gbhk': 'great big hug & kiss',\n",
              " 'gd&r': 'grinning, ducking, and running',\n",
              " 'gf': 'girlfriend',\n",
              " 'gfn': 'gone for now',\n",
              " 'gg': 'good game',\n",
              " 'ggp': 'gotta go pee',\n",
              " 'gi': 'good idea',\n",
              " 'giwisi': 'gee, i wish i said it',\n",
              " 'giwist': \"gee, i wish i'd said that\",\n",
              " 'gj': 'good job',\n",
              " 'gl': 'good luck',\n",
              " 'gm': 'good match',\n",
              " 'gmab': 'give me a break',\n",
              " 'gmta': 'great minds think alike',\n",
              " 'gn': 'good night',\n",
              " 'gr8': 'great',\n",
              " 'gtg': 'got to go',\n",
              " 'gtrm': 'going to read mail',\n",
              " 'gtsy': 'glad to see you',\n",
              " 'h&k': 'hugs and kisses',\n",
              " 'hagd': 'have a good day',\n",
              " 'hagn': 'have a good night',\n",
              " 'hcit': 'how cool is that',\n",
              " 'hf': 'have fun',\n",
              " 'hhis': 'hanging head in shame',\n",
              " 'hhoj': 'ha ha only joking',\n",
              " 'hhok': 'ha ha only kidding',\n",
              " 'hig': \"how's it going\",\n",
              " 'hoas': 'hold on a second',\n",
              " 'ht': 'hi there',\n",
              " 'hth': 'hope this helps',\n",
              " 'iac': 'in any case',\n",
              " 'iag': \"it's all good\",\n",
              " 'iagw': 'in a good way',\n",
              " 'ic': 'i see / in character',\n",
              " 'idgi': \"i don't get it\",\n",
              " 'iggp': 'i gotta go pee',\n",
              " 'igp': 'i gotta pee',\n",
              " 'iha': 'i hate acronyms',\n",
              " 'ihu': 'i hear you',\n",
              " 'iirc': 'if i remember correctly',\n",
              " 'ik': 'i know',\n",
              " 'ikwum': 'i know what you mean',\n",
              " 'ilu': 'i love you',\n",
              " 'ily': 'i love you',\n",
              " 'im': 'i am',\n",
              " 'imao': 'in my arrogant opinion',\n",
              " 'imco': 'in my considered opinion',\n",
              " 'ime': 'in my experience.',\n",
              " 'imnerho': 'in my not even remotely humble opinion',\n",
              " 'ims': 'i am sorry',\n",
              " 'inrs': \"it's not rocket science\",\n",
              " 'ioh': \"i'm outta here\",\n",
              " 'ipn': \"i'm posting naked\",\n",
              " 'irstbo': 'it really sucks the big one',\n",
              " 'iswym': 'i see what you mean',\n",
              " 'itigbs': \"i think i'm going to be sick\",\n",
              " 'iwalu': 'i will always love you',\n",
              " 'j/p': 'just playing',\n",
              " 'j4g': 'just for grins',\n",
              " 'jbod': 'just a bunch of disks',\n",
              " 'jic': 'just in case',\n",
              " 'jk': 'just kidding',\n",
              " 'jmho': 'just my humble opinion',\n",
              " 'jmo': 'just my opinion',\n",
              " 'jtlyk': 'just to let you know',\n",
              " 'kewl': 'cool',\n",
              " 'kir': 'keeping it real',\n",
              " 'kotl': 'kiss on the lips',\n",
              " 'kwim': 'know what i mean?',\n",
              " 'l8r g8r': 'later gator',\n",
              " 'ld': 'later, dude',\n",
              " 'ldr': 'long-distance relationship',\n",
              " 'lho': 'laughing head off',\n",
              " 'llta': 'lots and lots of thunderous applause',\n",
              " 'lmk': 'let me know',\n",
              " 'lmso': 'laughing my socks off',\n",
              " 'lrf': 'little rubber feet',\n",
              " 'lshmbh': 'laughing so hard my belly hurts',\n",
              " 'ltm': 'laugh to myself',\n",
              " 'ltns': 'long time no see',\n",
              " 'ltr': 'long-term relationship',\n",
              " 'lulas': 'love you like a sister',\n",
              " 'luwamh': 'love you with all my heart',\n",
              " 'ly4e': 'love ya forever',\n",
              " 'lyk': 'let you know',\n",
              " 'm8': 'mate',\n",
              " 'me2': 'me too',\n",
              " 'msg': 'message',\n",
              " 'mtf': 'more to follow',\n",
              " 'musm': 'miss you so much',\n",
              " 'myob': 'mind your own business',\n",
              " 'n/c': 'not cool',\n",
              " 'n/p': 'no problem',\n",
              " 'n1': 'nice one',\n",
              " 'n2m': 'not too much',\n",
              " 'nadt': 'not a darn thing',\n",
              " 'nbd': 'no big deal',\n",
              " 'ne1': 'anyone',\n",
              " 'nfg': 'no f*cking good',\n",
              " 'nm': 'not much',\n",
              " 'nmh': 'not much here',\n",
              " 'nmjc': 'nothing much, just chillin',\n",
              " 'nmp': 'not my problem',\n",
              " 'nnito': 'not necessarily in that order',\n",
              " 'no1': 'no one',\n",
              " 'nottomh': 'not off the top of my head',\n",
              " 'noyb': 'none of your business',\n",
              " 'np': 'no problem',\n",
              " 'nuff': 'enough said',\n",
              " 'nw': 'no way',\n",
              " 'oll': 'online love',\n",
              " 'omdb': 'over my dead body',\n",
              " 'omg': 'oh my gosh',\n",
              " 'onna': 'oh no, not again',\n",
              " 'ooc': 'out of character',\n",
              " 'ooto': 'out of the office',\n",
              " 'ot': 'off topic',\n",
              " 'otf': 'off the floor',\n",
              " 'otth': 'on the third hand',\n",
              " 'ottomh': 'off the top of my head',\n",
              " 'p911': 'parent alert',\n",
              " 'pans': 'pretty awesome new stuff',\n",
              " 'pda': 'public display of affection',\n",
              " 'pebcak': 'problem exists between chair and keyboard',\n",
              " 'pibkac': 'problem is between keyboard and chair',\n",
              " 'pls': 'please',\n",
              " 'plz': 'please',\n",
              " 'pm': 'private message',\n",
              " 'pmfjib': 'pardon me for jumping in but...',\n",
              " 'poahf': 'put on a happy face',\n",
              " 'pos': 'parents over shoulder',\n",
              " 'pots': 'plain old telephone service',\n",
              " 'ppl': 'people',\n",
              " 'ql': 'quit laughing',\n",
              " 'qt': 'cutie',\n",
              " 'rbtl': 'reading between the lines',\n",
              " 'resq': 'rescue',\n",
              " 'rfc': 'request for comment',\n",
              " 'rl': 'real life - i.e. when not chatting',\n",
              " 'rofl': 'roll on floor laughing',\n",
              " 'ror': 'raffing out roud (from scoobydoo = \"laughing out loud\")',\n",
              " 'rotflmbo': 'rolling on the floor laughing my butt off',\n",
              " 'rpg': 'role-playing games',\n",
              " 'rt': 'real time',\n",
              " 'rtm': 'read the manual',\n",
              " 'ryo': 'roll your own',\n",
              " 's2r': 'send to receive',\n",
              " 's4l': 'spam for life',\n",
              " 'scnr': 'sorry, could not resist',\n",
              " 'sete': 'smiling ear to ear',\n",
              " 'shcoon': 'shoot hot coffee out of nose',\n",
              " 'shid': 'slaps head in disgust',\n",
              " 'smem': 'send me an email',\n",
              " 'smim': 'send me an instant message',\n",
              " 'sohf': 'sense of humour failure',\n",
              " 'somy': 'sick of me yet?',\n",
              " 'sos': 'so as',\n",
              " 'sotmg': 'short on time must go',\n",
              " 'ssdd': 'same stuff different day',\n",
              " 'stfw': 'search the f*cking web',\n",
              " 'str8': 'straight',\n",
              " 'stw': 'search the web',\n",
              " 'su': 'shut up',\n",
              " 'suakm': 'shut up and kiss me',\n",
              " 'swak': 'sealed with a kiss',\n",
              " 'swalk': 'sealed with all loving kiss',\n",
              " 'swl': 'screaming with laughter',\n",
              " 'sys': 'see you soon',\n",
              " 's^': \"s'up - what's up\",\n",
              " 'tafn': \"that's all for now\",\n",
              " 'tah': 'take a hike',\n",
              " 'tbc': 'to be continued',\n",
              " 'tco': 'taken care of',\n",
              " 'tcoy': 'take care of yourself',\n",
              " 'thx': 'thanks',\n",
              " 'tilii': 'tell it like it is',\n",
              " 'tlk2ul8r': 'talk to you later',\n",
              " 'tm': 'trust me',\n",
              " 'tma': 'take my advice',\n",
              " 'tmi': 'too much information',\n",
              " 'tnt': 'till next time',\n",
              " 'tnx': 'thanks',\n",
              " 'tnx 1.0e6': 'thanks a million',\n",
              " 'toh': 'the other half',\n",
              " 'tom': 'tomorrow',\n",
              " 'topca': 'till our paths cross again (early celtic chat term)',\n",
              " 'tptb': 'the powers that be',\n",
              " 'ttfn': 'ta-ta for now',\n",
              " 'ttg': 'time to go',\n",
              " 'ttyl': 'talk to you later',\n",
              " 'tvm': 'thank you very much',\n",
              " 'ty': 'thank you',\n",
              " 'tyvm': 'thank you very much',\n",
              " 'u2': 'you too',\n",
              " 'uapita': \"you're a pain in the ass\",\n",
              " 'uw': \"you're welcome\",\n",
              " 'vbg': 'very big grin',\n",
              " 'vbseg': 'very big s**t-eating grin',\n",
              " 'veg': 'very evil grin',\n",
              " 'ves': 'very evil smile',\n",
              " 'vm': 'voice mail',\n",
              " 'vwg': 'very wicked grin',\n",
              " 'vws': 'very wicked smile',\n",
              " 'w2f': 'way too funny',\n",
              " 'w8': 'wait',\n",
              " 'w8am': 'wait a minute',\n",
              " 'wayd': 'what are you doing',\n",
              " 'wb': 'write back',\n",
              " 'wbs': 'write back soon',\n",
              " 'wc': 'who cares?',\n",
              " 'wdalyic': 'who died and left you in charge?',\n",
              " 'weg': 'wicked evil grin',\n",
              " 'wf': 'way fun',\n",
              " 'wfm': 'works for me',\n",
              " 'wg': 'wicked grin',\n",
              " 'wibni': \"wouldn't it be nice if...\",\n",
              " 'wnditwb': 'we never did it this way before',\n",
              " 'wrt': 'with regard / respect to',\n",
              " 'wt?': 'what / who the ?',\n",
              " 'wtf': 'what the f*ck?',\n",
              " 'wtfo': 'what the f*ck? over!',\n",
              " 'wtg': 'way to go!',\n",
              " 'wtgp?': 'want to go private?',\n",
              " 'wth': 'what the hell?',\n",
              " 'wthdtm': 'what the hell does that mean?',\n",
              " 'wu?': \"what's up?\",\n",
              " 'wuf?': 'where are you from?',\n",
              " 'wyp': \"what's your problem?\",\n",
              " 'wysiwyg': 'what you see is what you get',\n",
              " 'wywh': 'wish you were here',\n",
              " 'xoxo': 'hugs and kisses',\n",
              " 'ybs': \"you'll be sorry\",\n",
              " 'ygbsm': \"you gotta be sh*ttin' me!\",\n",
              " 'ykybotlw': \"you know you've been on line too long when?\",\n",
              " 'ymmv': 'your mileage may vary',\n",
              " 'ymmvg': 'your mileage may vary greatly',\n",
              " 'yrg': 'you are good!',\n",
              " 'yw': \"you're welcome\",\n",
              " 'zzz': 'sleeping, bored',\n",
              " \"ain't\": 'is not',\n",
              " \"aren't\": 'are not',\n",
              " \"can't\": 'cannot',\n",
              " \"can't've\": 'cannot have',\n",
              " \"'cause\": 'because',\n",
              " \"could've\": 'could have',\n",
              " \"couldn't\": 'could not',\n",
              " \"couldn't've\": 'could not have',\n",
              " \"didn't\": 'did not',\n",
              " \"doesn't\": 'does not',\n",
              " \"don't\": 'do not',\n",
              " \"hadn't\": 'had not',\n",
              " \"hadn't've\": 'had not have',\n",
              " \"hasn't\": 'has not',\n",
              " \"haven't\": 'have not',\n",
              " \"he'd\": 'he would',\n",
              " \"he'd've\": 'he would have',\n",
              " \"he'll\": 'he will',\n",
              " \"he'll've\": 'he he will have',\n",
              " \"he's\": 'he is',\n",
              " \"how'd\": 'how did',\n",
              " \"how'd'y\": 'how do you',\n",
              " \"how'll\": 'how will',\n",
              " \"how's\": 'how is',\n",
              " \"I'd\": 'I would',\n",
              " \"I'd've\": 'I would have',\n",
              " \"I'll\": 'I will',\n",
              " \"I'll've\": 'I will have',\n",
              " \"I'm\": 'I am',\n",
              " \"I've\": 'I have',\n",
              " \"i'd\": 'i would',\n",
              " \"i'd've\": 'i would have',\n",
              " \"i'll\": 'i will',\n",
              " \"i'll've\": 'i will have',\n",
              " \"i'm\": 'i am',\n",
              " \"i've\": 'i have',\n",
              " \"isn't\": 'is not',\n",
              " \"it'd\": 'it would',\n",
              " \"it'd've\": 'it would have',\n",
              " \"it'll\": 'it will',\n",
              " \"it'll've\": 'it will have',\n",
              " \"it's\": 'it is',\n",
              " \"let's\": 'let us',\n",
              " \"ma'am\": 'madam',\n",
              " \"mayn't\": 'may not',\n",
              " \"might've\": 'might have',\n",
              " \"mightn't\": 'might not',\n",
              " \"mightn't've\": 'might not have',\n",
              " \"must've\": 'must have',\n",
              " \"mustn't\": 'must not',\n",
              " \"mustn't've\": 'must not have',\n",
              " \"needn't\": 'need not',\n",
              " \"needn't've\": 'need not have',\n",
              " \"o'clock\": 'of the clock',\n",
              " \"oughtn't\": 'ought not',\n",
              " \"oughtn't've\": 'ought not have',\n",
              " \"shan't\": 'shall not',\n",
              " \"sha'n't\": 'shall not',\n",
              " \"shan't've\": 'shall not have',\n",
              " \"she'd\": 'she would',\n",
              " \"she'd've\": 'she would have',\n",
              " \"she'll\": 'she will',\n",
              " \"she'll've\": 'she will have',\n",
              " \"she's\": 'she is',\n",
              " \"should've\": 'should have',\n",
              " \"shouldn't\": 'should not',\n",
              " \"shouldn't've\": 'should not have',\n",
              " \"so've\": 'so have',\n",
              " \"so's\": 'so as',\n",
              " \"that'd\": 'that would',\n",
              " \"that'd've\": 'that would have',\n",
              " \"that's\": 'that is',\n",
              " \"there'd\": 'there would',\n",
              " \"there'd've\": 'there would have',\n",
              " \"there's\": 'there is',\n",
              " \"they'd\": 'they would',\n",
              " \"they'd've\": 'they would have',\n",
              " \"they'll\": 'they will',\n",
              " \"they'll've\": 'they will have',\n",
              " \"they're\": 'they are',\n",
              " \"they've\": 'they have',\n",
              " \"to've\": 'to have',\n",
              " \"wasn't\": 'was not',\n",
              " \"we'd\": 'we would',\n",
              " \"we'd've\": 'we would have',\n",
              " \"we'll\": 'we will',\n",
              " \"we'll've\": 'we will have',\n",
              " \"we're\": 'we are',\n",
              " \"we've\": 'we have',\n",
              " \"weren't\": 'were not',\n",
              " \"what'll\": 'what will',\n",
              " \"what'll've\": 'what will have',\n",
              " \"what're\": 'what are',\n",
              " \"what's\": 'what is',\n",
              " \"what've\": 'what have',\n",
              " \"when's\": 'when is',\n",
              " \"when've\": 'when have',\n",
              " \"where'd\": 'where did',\n",
              " \"where's\": 'where is',\n",
              " \"where've\": 'where have',\n",
              " \"who'll\": 'who will',\n",
              " \"who'll've\": 'who will have',\n",
              " \"who's\": 'who is',\n",
              " \"who've\": 'who have',\n",
              " \"why's\": 'why is',\n",
              " \"why've\": 'why have',\n",
              " \"will've\": 'will have',\n",
              " \"won't\": 'will not',\n",
              " \"won't've\": 'will not have',\n",
              " \"would've\": 'would have',\n",
              " \"wouldn't\": 'would not',\n",
              " \"wouldn't've\": 'would not have',\n",
              " \"y'all\": 'you all',\n",
              " \"y'all'd\": 'you all would',\n",
              " \"y'all'd've\": 'you all would have',\n",
              " \"y'all're\": 'you all are',\n",
              " \"y'all've\": 'you all have',\n",
              " \"you'd\": 'you would',\n",
              " \"you'd've\": 'you would have',\n",
              " \"you'll\": 'you will',\n",
              " \"you'll've\": 'you will have',\n",
              " \"you're\": 'you are',\n",
              " \"you've\": 'you have',\n",
              " 'aint': 'is not',\n",
              " 'arent': 'are not',\n",
              " 'cant': 'cannot',\n",
              " 'cantve': 'cannot have',\n",
              " 'cause': 'because',\n",
              " 'couldve': 'could have',\n",
              " 'couldnt': 'could not',\n",
              " 'couldntve': 'could not have',\n",
              " 'didnt': 'did not',\n",
              " 'doesnt': 'does not',\n",
              " 'dont': 'do not',\n",
              " 'hadnt': 'had not',\n",
              " 'hadntve': 'had not have',\n",
              " 'hasnt': 'has not',\n",
              " 'havent': 'have not',\n",
              " 'hed': 'he would',\n",
              " 'hedve': 'he would have',\n",
              " 'hell': 'he will',\n",
              " 'hellve': 'he he will have',\n",
              " 'hes': 'he is',\n",
              " 'howd': 'how did',\n",
              " 'howdy': 'how do you',\n",
              " 'howll': 'how will',\n",
              " 'hows': 'how is',\n",
              " 'Id': 'I would',\n",
              " 'Idve': 'I would have',\n",
              " 'Ill': 'I will',\n",
              " 'Illve': 'I will have',\n",
              " 'Im': 'I am',\n",
              " 'Ive': 'I have',\n",
              " 'id': 'i would',\n",
              " 'idve': 'i would have',\n",
              " 'ill': 'i will',\n",
              " 'illve': 'i will have',\n",
              " 'ive': 'i have',\n",
              " 'isnt': 'is not',\n",
              " 'itd': 'it would',\n",
              " 'itdve': 'it would have',\n",
              " 'itll': 'it will',\n",
              " 'itllve': 'it will have',\n",
              " 'its': 'it is',\n",
              " 'lets': 'let us',\n",
              " 'maam': 'madam',\n",
              " 'maynt': 'may not',\n",
              " 'mightve': 'might have',\n",
              " 'mightnt': 'might not',\n",
              " 'mightntve': 'might not have',\n",
              " 'mustve': 'must have',\n",
              " 'mustnt': 'must not',\n",
              " 'mustntve': 'must not have',\n",
              " 'neednt': 'need not',\n",
              " 'needntve': 'need not have',\n",
              " 'oclock': 'of the clock',\n",
              " 'oughtnt': 'ought not',\n",
              " 'oughtntve': 'ought not have',\n",
              " 'shant': 'shall not',\n",
              " 'shantve': 'shall not have',\n",
              " 'shed': 'she would',\n",
              " 'shedve': 'she would have',\n",
              " 'shell': 'she will',\n",
              " 'shellve': 'she will have',\n",
              " 'shes': 'she is',\n",
              " 'shouldve': 'should have',\n",
              " 'shouldnt': 'should not',\n",
              " 'shouldntve': 'should not have',\n",
              " 'sove': 'so have',\n",
              " 'thatd': 'that would',\n",
              " 'thatdve': 'that would have',\n",
              " 'thats': 'that is',\n",
              " 'thered': 'there would',\n",
              " 'theredve': 'there would have',\n",
              " 'theres': 'there is',\n",
              " 'theyd': 'they would',\n",
              " 'theydve': 'they would have',\n",
              " 'theyll': 'they will',\n",
              " 'theyllve': 'they will have',\n",
              " 'theyre': 'they are',\n",
              " 'theyve': 'they have',\n",
              " 'tove': 'to have',\n",
              " 'wasnt': 'was not',\n",
              " 'wed': 'we would',\n",
              " 'wedve': 'we would have',\n",
              " 'well': 'we will',\n",
              " 'wellve': 'we will have',\n",
              " 'were': 'we are',\n",
              " 'weve': 'we have',\n",
              " 'werent': 'were not',\n",
              " 'whatll': 'what will',\n",
              " 'whatllve': 'what will have',\n",
              " 'whatre': 'what are',\n",
              " 'whats': 'what is',\n",
              " 'whatve': 'what have',\n",
              " 'whens': 'when is',\n",
              " 'whenve': 'when have',\n",
              " 'whered': 'where did',\n",
              " 'wheres': 'where is',\n",
              " 'whereve': 'where have',\n",
              " 'wholl': 'who will',\n",
              " 'whollve': 'who will have',\n",
              " 'whos': 'who is',\n",
              " 'whove': 'who have',\n",
              " 'whys': 'why is',\n",
              " 'whyve': 'why have',\n",
              " 'willve': 'will have',\n",
              " 'wont': 'will not',\n",
              " 'wontve': 'will not have',\n",
              " 'wouldve': 'would have',\n",
              " 'wouldnt': 'would not',\n",
              " 'wouldntve': 'would not have',\n",
              " 'yall': 'you all',\n",
              " 'yalld': 'you all would',\n",
              " 'yalldve': 'you all would have',\n",
              " 'yallre': 'you all are',\n",
              " 'yallve': 'you all have',\n",
              " 'youd': 'you would',\n",
              " 'youdve': 'you would have',\n",
              " 'youll': 'you will',\n",
              " 'youllve': 'you will have',\n",
              " 'youre': 'you are',\n",
              " 'youve': 'you have'}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DICT = {**ACRONYMS_MAP, **CONTRACTION_MAP}\n",
        "DICT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdzAHDodf0BT"
      },
      "source": [
        "La parte principale di preprocessing avviene grazie alla funzione <i>process_tweet</i>, che va ad eseguire una serie di operazioni a cascata sul singolo tweet. <br>\n",
        "Inoltre, vado a definire anche altre due funzioni:\n",
        "1. <i>remove_emoji</i> -> necessaria per la rimozione di eventuali emoji\n",
        "2. <i>convert_abbreviations</i> -> sfrutta il dizionario creato in precedenza per effettuare tutte le sostituzioni. <br> <br>\n",
        "\n",
        "Dentro la funzione *process_tweet* è presente la *rimozione delle stopwords* e la *lemmatizzazione*, ovvero quel processo che permette di ridurre un termine alla sua forma base: ogni tweet viene quindi scomposto in *token*, e se la lunghezza del token è maggiore di 1, allora applica la lemmatizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StYd5Ay_f0BU",
        "outputId": "113eda79-01c1-4219-b58b-b947dcfed304"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\danil\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "url_pattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\" # Regex per il matching di URL con http, https e www\n",
        "username_pattern = '@[^\\s]+' # Regex per il matching degli username (@Username01)\n",
        "nltk.download('stopwords') # Caricamento stopwords\n",
        "stop_words = set(stopwords.words('english')) # Definizione stopwords\n",
        "\n",
        "'''\n",
        "    Rimuove gli eventuali emoji\n",
        "'''\n",
        "\n",
        "def remove_emoji(string):\n",
        "    return emoji.get_emoji_regexp().sub(u'', string)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Applica delle operazioni a cascata al tweet\n",
        "'''\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    tweet = tweet.lower() # Tutto in minuscolo\n",
        "    tweet = html.unescape(tweet) # Elimino eventuali caratteri html, come ad esempio &amp e altri\n",
        "    tweet = re.sub(url_pattern, '', tweet) # Rimuovo gli URL\n",
        "    tweet = re.sub(username_pattern, '', tweet) # Rimuovo gli username\n",
        "    tweet = remove_emoji(tweet) # Rimuovo gli emoji\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Rimuovo la punteggiatura\n",
        "    tweet = re.sub('[^a-zA-Z0-9 \\n\\.]', '', tweet) # Rimuovo i numeri\n",
        "    tweet = re.sub(r'\\d+', '', tweet) # Rimuovo altri caratteri\n",
        "    tweet = re.sub(r\"(\\d+)(st|nd|rd|th)\\b\", r\"\\1\", tweet) # Rimuovo i suffissi dei numeri ordinali\n",
        "    tokens = word_tokenize(str(tweet)) # Tokenizzazione\n",
        "    final_tokens = [w for w in tokens if w not in stop_words] # Rimozione delle stopwords\n",
        "    lemmatizer = WordNetLemmatizer() # Lemmatizzazione\n",
        "    final_tweet = []\n",
        "    for w in final_tokens:\n",
        "        if len(w)>1:\n",
        "            word = lemmatizer.lemmatize(w)\n",
        "            final_tweet.append(word)\n",
        "    return ' '.join(final_tokens)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Converte le abbreviazioni presenti nella frase sfruttando il dizionario DICT:\n",
        "    controlla se la parola presente nella frase è presente come chiave del dizionario: se presente, effettua la sostituzione\n",
        "    con il rispettivo valore, altrimenti la mantiene\n",
        "'''\n",
        "\n",
        "def convert_abbreviations(tweet):\n",
        "    w = []\n",
        "    words = tweet.split()\n",
        "    t = [DICT[w] if w in DICT.keys() else w for w in words]\n",
        "    return ' '.join(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNBDsjcYf0BV"
      },
      "source": [
        " Per ogni tweet del DataFrame, applico tramite delle *lambda* prima la funzione *convert_abbreviations* e successivamente la funzione *process_tweet*; per ognuna vado a misurare il tempo impiegato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAdj0VdCf0BV",
        "outputId": "f4bcbc6d-ea2b-45af-d784-f76ffcca6b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abbreviations converted in 0.04 seconds\n",
            "Tweets processed in 4.97 seconds\n"
          ]
        }
      ],
      "source": [
        "start = timer()\n",
        "df['processed_tweets'] = df['tweet'].apply(lambda x: convert_abbreviations(x))\n",
        "end = timer()\n",
        "print(f'Abbreviations converted in {round((end-start),2)} seconds')\n",
        "\n",
        "start = timer()\n",
        "df['processed_tweets'] = df['tweet'].apply(lambda x: process_tweet(x))\n",
        "end = timer()\n",
        "print(f'Tweets processed in {round((end-start),2)} seconds') # circa 25 secondi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRb0G4OTf0BW"
      },
      "source": [
        "Per la classificazione (*supervisionata*) sono necessari:\n",
        "* Training set\n",
        "* Test set\n",
        "\n",
        "La funzione *get_datasets_with_TFIDF_VECTORIZER* permette di creare training set e test set a partire dal DataFrame, dove ogni tweet viene convertito in *vettore* utilizzando lo schema di pesatura ***TF-IDF***\n",
        "\n",
        "<br> Più nel dettaglio:\n",
        "1. Istanzio il *tf-idf vectorizer*, con l'opzione *use_idf = True*\n",
        "2. Le *features* ***X*** sono date da ogni tweet processato tramite tf-idf, mentre le *labels* ***y*** sono date dai valori di sentiment: vado quindi ad estrarre X e y dal DataFrame\n",
        "3. Tramite la funzione *train_test_split* di *scikit-learn*, splitto il DataFrame in training e test set, considerando come test set il 20% del DataFrame\n",
        "4. Tramite la libreria *pickle* (libreria che permette di *serializzare* e quindi di salvare oggetti) salvo il tf-idf vectorizer\n",
        "5. Restituisco features e labels sia del training che del test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwA4GUlVf0BW"
      },
      "outputs": [],
      "source": [
        "def get_datasets_with_TFIDF_VECTORIZER(df):\n",
        "    tfidfvectorizer = TfidfVectorizer(use_idf = True)\n",
        "    X = tfidfvectorizer.fit_transform(df['processed_tweets'].values.astype('U')) ## Values.astype ('U') serve al tf-idf vectorizer\n",
        "    y = df['sentiment']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20)\n",
        "    with open('tfidf_vectorizer.pk', 'wb') as f:\n",
        "        pickle.dump(tfidfvectorizer,f)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgO7YlAf0BW"
      },
      "source": [
        "Tramite il **tf-idf**, si ha a disposizione una rappresentazione numerica dei tweet: a questo punto è possibile applicare i vari algoritmi di classificazione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN8AqXGff0BX"
      },
      "source": [
        "<h1>Logistic Regression</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5RG5eU-f0BX"
      },
      "source": [
        "Prima di applicare la *regressione logistica*, vado ad effettuare uno *shuffle* del DataFrame tramite la funzione *sample* di Pandas, dopodichè istanzio il modello, creo i vari dataset tramite la funzione *get_datasets_with_TFIDF_VECTORIZER* definita in precedenza, addestro il modello e tramite la libreria *pickle* lo salvo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHnVBgy5f0BY"
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1)\n",
        "model = LogisticRegression(max_iter=50000)\n",
        "X_train, X_test, y_train, y_test = get_datasets_with_TFIDF_VECTORIZER(df)\n",
        "model.fit(X_train, y_train)\n",
        "with open('logistic_regression.pk','wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44QrpBuDf0BY"
      },
      "source": [
        "A questo punto calcolo le predizioni, (ovviamente calcolate usando il test set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmi-tggbf0BZ"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG9Am0Ctf0Ba"
      },
      "source": [
        "E calcolo le seguenti metriche:\n",
        "* Matrice di confusione\n",
        "* Accuracy\n",
        "* Precision\n",
        "\n",
        "E salvo l'accuracy per utilizzarla successivamente nella *web app Streamlit* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWxPq24If0Ba",
        "outputId": "0d06aac4-acda-4508-cdf8-9718519f49ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            " [[0.82058824 0.05294118 0.01470588 0.02941176 0.08235294]\n",
            " [0.01923077 0.875      0.00480769 0.0625     0.03846154]\n",
            " [0.01898734 0.07911392 0.81962025 0.01582278 0.0664557 ]\n",
            " [0.04832714 0.11524164 0.02230483 0.73977695 0.07434944]\n",
            " [0.03870968 0.23225806 0.03225806 0.03225806 0.66451613]] \n",
            "\n",
            " Accuracy: 0.791641429436705 \n",
            "\n",
            " Precision: 0.791641429436705\n"
          ]
        }
      ],
      "source": [
        "mat_conf = confusion_matrix(y_test, predictions,normalize='true')\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "prec = precision_score(y_test, predictions,average='micro')\n",
        "\n",
        "print(f'Confusion Matrix: \\n {mat_conf} \\n\\n Accuracy: {acc} \\n\\n Precision: {prec}')\n",
        "\n",
        "with open('logistic_regression_accuracy.pk', 'wb') as f:\n",
        "    pickle.dump(acc, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShDYui_ef0Bb"
      },
      "source": [
        "A questo punto, avendo a disposizione sia il *tf-idf vectorizer* che il modello, si possono effettuare delle predizioni al di fuori del test set: come prima cosa vado a caricare sia il vectorizer che il modello:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnmJrvFXf0Bb"
      },
      "outputs": [],
      "source": [
        "with open('tfidf_vectorizer.pk','rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "    \n",
        "with open('logistic_regression.pk','rb') as f:\n",
        "    logistic_regression = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiJ3WGHBf0Bc"
      },
      "source": [
        "Ovviamente il classificatore non restituisce il sentiment vero e proprio, ma solamente un valore intero: per poter ottenere il sentiment vero e proprio, vado a creare un dizionario che ne effettua il mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8kVDROZf0Bc"
      },
      "outputs": [],
      "source": [
        "classes = {0 : 'Anger', 1 : 'Fear',2 : 'Joy', 3 : 'Sadness',4 : 'Neutral'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD7vysY4f0Bc"
      },
      "source": [
        "Inoltre, per poter effettuare la predizione su una nuova frase, ovviamente il modello non riesce a riconoscere direttamente delle stringhe, ma bensì dei vettori tf-idf: per questo motivo, la query viene prima \"tradotta\" in vettore tf-idf usando il *vectorizer* creato in precedenza e successivamente si può effettuare la predizione:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcj2T_7Df0Bd",
        "outputId": "aaa97221-a4f7-4a14-d7fb-d92628ba6cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: i'm bored, Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "query = \"i'm bored\"\n",
        "transformed_query = tfidf_vectorizer.transform([query])\n",
        "pred = logistic_regression.predict(transformed_query)\n",
        "print(f\"Query: {query}, Sentiment: {classes[pred[0]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAZz7wx9f0Bd"
      },
      "source": [
        "Per il *classificatore bayesiano* il procedimento adottato è lo stesso della regressione logistica, con la differenza per l'appunto del modello utilizzato"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuTBgUGEf0Be"
      },
      "source": [
        "<h1>Naive Bayes</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goFdWaXrf0Be",
        "outputId": "4b009e2c-2ad3-4465-a20d-005ca95bad22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix: \n",
            " [[0.90153846 0.02461538 0.02153846 0.03384615 0.01846154]\n",
            " [0.03645833 0.859375   0.02083333 0.0546875  0.02864583]\n",
            " [0.01612903 0.02258065 0.92580645 0.01612903 0.01935484]\n",
            " [0.06953642 0.0794702  0.03311258 0.79801325 0.01986755]\n",
            " [0.12727273 0.13636364 0.07878788 0.05757576 0.6       ]] \n",
            "\n",
            " Accuracy: 0.8170805572380375 \n",
            "\n",
            " Precision: 0.8170805572380375\n",
            "Query: i'm bored, Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "df = df.sample(frac=1)\n",
        "model = ComplementNB()\n",
        "X_train, X_test, y_train, y_test = get_datasets_with_TFIDF_VECTORIZER(df)\n",
        "model.fit(X_train, y_train)\n",
        "with open('naive_bayes.pk','wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "    \n",
        "    \n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "mat_conf = confusion_matrix(y_test, predictions,normalize='true')\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "prec = precision_score(y_test, predictions,average='micro')\n",
        "\n",
        "print(f'Confusion Matrix: \\n {mat_conf} \\n\\n Accuracy: {acc} \\n\\n Precision: {prec}')\n",
        "\n",
        "with open('naive_bayes_accuracy.pk', 'wb') as f:\n",
        "    pickle.dump(acc, f)\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "\n",
        "with open('tfidf_vectorizer.pk','rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "    \n",
        "with open('naive_bayes.pk','rb') as f:\n",
        "    naive_bayes = pickle.load(f)\n",
        "    \n",
        "    \n",
        "classes = {0 : 'Anger', 1 : 'Fear',2 : 'Joy', 3 : 'Sadness',4 : 'Neutral'}\n",
        "\n",
        "\n",
        "query = \"i'm bored\"\n",
        "transformed_query = tfidf_vectorizer.transform([query])\n",
        "pred = naive_bayes.predict(transformed_query)\n",
        "print(f\"Query: {query}, Sentiment: {classes[pred[0]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBWtB99af0Bf"
      },
      "source": [
        "N.B. In questo specifico caso, tipicamente il Naive Bayes performa meglio rispetto alla regressione logistica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSnc9g3sf0Bf"
      },
      "source": [
        "<h1>Neural Network</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly_J7Riqf0Bf"
      },
      "source": [
        "Per mostrare che è possibile effettaure classificazione sfruttando diversi metodi e modelli (e anche per \"sfizio personale\"), vado a realizzare una *semplicissima* rete neurale tramite l'utilizzo della libreria *TensorFlow*: come primo step, creo i training e test set sempre tramite la funzione *get_datasets_with_TFIDF_VECTORIZER*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaX-ug7wf0Bf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = get_datasets_with_TFIDF_VECTORIZER(df)\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8S5E11-f0Bg"
      },
      "source": [
        "E stampo lo *shape*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6SUkBwEf0Bg",
        "outputId": "8c094779-d267-4646-a30c-f3a776570f39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13953,)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shape = X_train[0].shape\n",
        "shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF2n9Ma2f0Bg"
      },
      "source": [
        "Creo la rete andando a \"stackare\" i layers:\n",
        "* Il primo con 128 unità\n",
        "* 64 unità\n",
        "* 32 unità\n",
        "* Ovviamente il *layer* di output è composto da 5 uscite, che è pari al numero delle classi\n",
        "\n",
        "Tutti i livello hanno come funzione di attivazione la ReLU. <br> <br>\n",
        "\n",
        "Dopodichè definisco anche la funzione di *loss* e creo il modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L46l1WCOf0Bh",
        "outputId": "0225aa97-9412-4044-9f78-53b14d66df0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 13953)             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               1786112   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 165       \n",
            "=================================================================\n",
            "Total params: 1,796,613\n",
            "Trainable params: 1,796,613\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=shape))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(5))\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OqNqGXUf0Bh"
      },
      "source": [
        "Addestro il modello con 20 epoche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-7qMBr8f0Bi",
        "outputId": "b72350da-61de-4c08-cccb-dc6df75358b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "207/207 [==============================] - 3s 11ms/step - loss: 1.2116 - accuracy: 0.5174\n",
            "Epoch 2/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.3213 - accuracy: 0.9101\n",
            "Epoch 3/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.1120 - accuracy: 0.9635\n",
            "Epoch 4/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0740 - accuracy: 0.9696\n",
            "Epoch 5/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0581 - accuracy: 0.9691\n",
            "Epoch 6/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0515 - accuracy: 0.9737\n",
            "Epoch 7/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0481 - accuracy: 0.9724\n",
            "Epoch 8/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0431 - accuracy: 0.9743\n",
            "Epoch 9/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0414 - accuracy: 0.9762\n",
            "Epoch 10/20\n",
            "207/207 [==============================] - 2s 12ms/step - loss: 0.0387 - accuracy: 0.9750\n",
            "Epoch 11/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0376 - accuracy: 0.9740\n",
            "Epoch 12/20\n",
            "207/207 [==============================] - 2s 12ms/step - loss: 0.0356 - accuracy: 0.9765\n",
            "Epoch 13/20\n",
            "207/207 [==============================] - 2s 12ms/step - loss: 0.0349 - accuracy: 0.9768\n",
            "Epoch 14/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0356 - accuracy: 0.9747\n",
            "Epoch 15/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0339 - accuracy: 0.9756 0s - l\n",
            "Epoch 16/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0344 - accuracy: 0.9777\n",
            "Epoch 17/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0338 - accuracy: 0.9762\n",
            "Epoch 18/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0344 - accuracy: 0.9743\n",
            "Epoch 19/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0328 - accuracy: 0.9779\n",
            "Epoch 20/20\n",
            "207/207 [==============================] - 2s 11ms/step - loss: 0.0331 - accuracy: 0.9768\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x187b51b3448>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipliZLFif0Bi"
      },
      "source": [
        "Faccio la validazione del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1VUeps5f0Bi",
        "outputId": "82057c59-a68b-4c4f-85fa-01d78aa79aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52/52 - 0s - loss: 0.8534 - accuracy: 0.7953\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.8533831238746643, 0.7952755689620972]"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1xeuQWKf0Bj",
        "outputId": "c2f24234-c941-41dd-fe0c-fd9a0fd9678c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52/52 - 0s - loss: 0.8534 - accuracy: 0.7953\n"
          ]
        }
      ],
      "source": [
        "loss,acc = model.evaluate(X_test,  y_test, verbose=2)\n",
        "\n",
        "with open('nn_loss.pk', 'wb') as f:\n",
        "    pickle.dump(loss, f)\n",
        "    \n",
        "with open('nn_accuracy.pk', 'wb') as f:\n",
        "    pickle.dump(acc, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoqdGQF9f0Bj"
      },
      "source": [
        "Salvo il modello\n",
        "N.B: I modello di TensorFlow non possono essere salvati in formato *.pickle* bensì *.h5*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bozhfvMHf0Bk"
      },
      "outputs": [],
      "source": [
        "model.save('neural_network.h5')\n",
        "# model = load_model('neural_network.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJRsWQ5f0Bk"
      },
      "source": [
        "Per fare in modo che il modello restituisca delle probabilità, lo inserisco in un layer *Softmax*, e dopodichè calcolo le predizioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOjDD6O7f0Bk",
        "outputId": "6ca94cc8-5bd7-443f-c19a-d55d6366e146"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\danil\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ]
        }
      ],
      "source": [
        "prob_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
        "y_pred = prob_model.predict([X_test])\n",
        "y_pred_classes = prob_model.predict_classes([X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTjEDMt2f0Bl"
      },
      "source": [
        "A partire dalle predizioni, vado a creare un report partendo dalla funzione *classification_report* di *scikit-learn* per poi creare un dizionario contenente come chiave la classe e come valora l'*accuracy* ottenuta per ogni classe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yZS5TJpf0Bl"
      },
      "outputs": [],
      "source": [
        "report = classification_report(y_test, y_pred_classes, output_dict = True)\n",
        "keys = list(report.keys())\n",
        "values = list(report.values())\n",
        "classes = keys[:-3]\n",
        "accuracy_per_class = []\n",
        "for i in range(len(classes)):\n",
        "    accuracy_per_class.append(values[i]['recall'])\n",
        "x = classes\n",
        "y = []\n",
        "for i in accuracy_per_class:\n",
        "    y.append(round(i*100,2))\n",
        "acc_per_class = dict(zip(x,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Yww1W3f0Bm",
        "outputId": "bfbd35aa-43d1-45b5-a70b-daf0afcc37d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': 85.29, '1': 80.48, '2': 78.48, '3': 73.7, '4': 79.41}"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1Y_fFddf0Bm"
      },
      "source": [
        "A partire dalle accuracy ottenute per ciascuna classe, vado a creare un *istogramma*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAeCRZY5f0Bm",
        "outputId": "f3e70ca2-fe7b-4610-ae31-24438ebba338"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkoAAAJNCAYAAADOJlohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5SddX3v8c+Pi1yMIi2XkiAJhnCRJIaQClgPCK0oF4sgCPEucFysIy3GYyldXvCgFlCXKEVABQyKh4jQQo4WFUMUlig4keCFi7SCEkLlbjARQ5Lf+WOGNIH8yEazZ+8Mr9das2aeZ/bz+N1rL4d3nufZzy611gAA8HQb9HoAAIB+JZQAABqEEgBAg1ACAGgQSgAADUIJAKBho27sdKuttqrjxo3rxq4BANapefPmPVhr3XpNv+tKKI0bNy4DAwPd2DUAwDpVSvlV63dOvQEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKz9JZZ52V3XffPRMnTsz06dPz+OOP5x3veEd23HHHTJkyJVOmTMn8+fOftt38+fOzzz77ZPfdd8/kyZPz1a9+deXvrr322kydOjUTJ07M29/+9ixbtmw4nxIA0FBqret8p9OmTasDAwPrfL+9du+99+aVr3xlbr311my22WZ54xvfmIMPPjjf/e53c+ihh+bII49sbvuLX/wipZRMmDAhCxcuzJ577pnbbrstL3zhCzN27NjMmTMnO++8cz70oQ9l7NixOe6444bxmQHAc1cpZV6tddqafueI0rO0bNmy/P73v8+yZcuyZMmSjB49uqPtdt5550yYMCFJMnr06GyzzTZ54IEH8tBDD2WTTTbJzjvvnCR59atfnSuuuKJr8wMAnRNKz8KYMWPyvve9LzvssEO22267bLHFFjnwwAOTJO9///szefLkzJgxI3/4wx+ecT833XRTli5dmvHjx2errbbKE088kSePwF1++eW55557uv5cAIC1E0rPwiOPPJKrrroqd911VxYuXJjFixfnkksuyemnn57bb789P/rRj/Lwww/nzDPPbO7jvvvuy1vf+tZ88YtfzAYbbJBSSmbNmpUZM2bk5S9/eV7wghdko402GsZnBQC0CKVn4Tvf+U523HHHbL311tl4441zxBFH5IYbbsh2222XUko22WSTvPOd78xNN920xu0XLVqUQw45JB/96Eez9957r1y/zz775Prrr89NN92Ufffdd+UpOgCgt4TSs7DDDjvkhz/8YZYsWZJaa+bMmZPddtst9913X5Kk1porr7wyEydOfNq2S5cuzeGHH563ve1tOeqoo1b73f33358k+cMf/pAzzzwzJ5xwQvefDACwVkLpWdhrr71y5JFHZurUqZk0aVJWrFiRd73rXXnzm9+cSZMmZdKkSXnwwQfzgQ98IEkyMDCQ448/Pkly2WWX5brrrsvMmTOfdhuBT3ziE9ltt90yefLkvO51r8sBBxzQs+cIAPw3twcAAJ7T3B4AAOCPsF6/vWrcKd/o9Qgjyt1nHNLrEQCgrziiBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSI9JZZ52V3XffPRMnTsz06dPz+OOP56677spee+2VCRMm5Oijj87SpUub2//617/OqFGj8slPfvIZ9wnAyCaUGHHuvffenH322RkYGMjPfvazLF++PLNmzco//uM/ZsaMGbnzzjuz5ZZb5sILL2zuY8aMGTnooIPWuk8ARjahxIi0bNmy/P73v8+yZcuyZMmSbLfddrn22mtz5JFHJkne/va358orr1zjtldeeWVe8pKXZPfdd3/GfY4ePbrrzwOA3hJKjDhjxozJ+973vuywww7ZbrvtssUWW2TPPffMi170omy00UZJku233z733nvv07ZdvHhxzjzzzJx66qlr3eeBBx44LM8HgN4RSow4jzzySK666qrcddddWbhwYRYvXpyrr776aY8rpTxt3amnnpoZM2Zk1KhRa93nJZdc0rXnAEB/2KjXA8C69p3vfCc77rhjtt566yTJEUcckRtuuCGPPvpoli1blo022igLFixY46mzG2+8MZdffnlOPvnkPProo9lggw2y6aabZtttt13jPt/ylrcM63MDYHgJJUacHXbYIT/84Q+zZMmSbLbZZpkzZ06mTZuW/fffP5dffnmOOeaYXHzxxTnssMOetu3111+/8ucPf/jDGTVqVE488cTceOONa9wnACObU2+MOHvttVeOPPLITJ06NZMmTcqKFSvyrne9K2eeeWY+9alPZaeddspDDz2U4447Lkkye/bsfOhDH/qj9gnQa3fccUemTJmy8uuFL3xhPv3pT+eWW27JPvvsk0mTJuV1r3tdFi1a1NzH8uXLs8cee+TQQw9due6cc87JTjvtlFJKHnzwweF4Kn2p1FrX+U6nTZtWBwYG1vl+n2rcKd/o+v/Gc8ndZxzS6xEA+BMsX748Y8aMyY033pgjjzwyn/zkJ7Pffvvloosuyl133ZWPfOQja9zuU5/6VAYGBrJo0aJ8/etfT5LcfPPN2XLLLfOqV70qAwMD2WqrrYbzqQyrUsq8WusaTxM4ogQAI8ScOXMyfvz4jB07NnfccUf23XffJMmrX/3qXHHFFWvcZsGCBfnGN76R448/frX1e+yxR8aNG9ftkfuea5ToKkf91j1H/oCWWbNmZfr06UmSiRMnZvbs2TnssMPyta99Lffcc88at3nPe96Tj3/843nssceGc9T1hiNKADACLF26NLNnz85RRx2VJLnooovy2c9+NnvuuWcee+yxPO95z3vaNl//+tezzTbbZM899xzucdcbQgnoidYFqPPnz8/ee++dKVOmZNq0abnpppua+1i0aFHGjBmTE088ceW6Sy+9NJMmTcrkyZPz2te+9jl9ESrPLVdffXWmTp2abbfdNkmy66675tvf/nbmzZuX6dOnZ/z48U/b5vvf/35mz56dcePG5Zhjjsm1117rtidPIZSAnthll10yf/78zJ8/P/Pmzcvmm2+eww8/PCeffHJOPfXUzJ8/P6eddlpOPvnk5j4++MEPZr/99lu5vGzZspx00kmZO3dufvKTn2Ty5Mk555xzhuPpQM9deumlK0+7Jcn999+fJFmxYkU++tGP5oQTTnjaNqeffnoWLFiQu+++O7NmzcoBBxzgZrpPIZSAnlv1AtRSysq3Mf/2t79tfqbevHnz8pvf/Ga1j5KptabWmsWLF6fWmkWLFvlMPp4TlixZkmuuuSZHHHHEynWXXnppdt555+y6664ZPXp03vnOdyZJFi5cmIMPPnit+zz77LOz/fbbZ8GCBZk8efLTLvZ+rnB7AFbqxkXCXqN1byRezH3sscdm6tSpOfHEE3PbbbflNa95TWqtWbFiRW644YaMHTt2tcevWLEiBxxwQL785S9nzpw5GRgYWHnk6PLLL8+xxx6b5z//+ZkwYULmzp2bDTfcsBdPC1hPuD0A0LeeegHqeeedl7POOiv33HNPzjrrrJU3Bl3Vueeem4MPPjgvfvGLV1v/xBNP5LzzzsvNN9+chQsXZvLkyTn99NOH5XkAI5PbAwA99dQLUC+++OJ85jOfSZIcddRRazzc/4Mf/CDXX399zj333Pzud7/L0qVLM2rUqLzhDW9IkpUXrb7xjW/MGWecMUzPBBxF74ZeH0UXSkBPPfUC1NGjR+d73/teXvWqV+Xaa6/NhAkTnrbNV77ylZU/z5w5MwMDAznjjDOycOHC3HrrrXnggQey9dZb55prrsluu+02LM8DGJmEEtAzT16A+rnPfW7lui984Qs56aSTsmzZsmy66ab5/Oc/nyQZGBjI+eefnwsuuKC5v9GjR+fUU0/Nvvvum4033jhjx47NzJkzu/00gBHMxdys5GLu9UOvD0MDbf7mrXvD8TfPxdwAAH8Ep94A/wpexxz1g5HDESUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANCwUa8HAGD9cMcdd+Too49eufzLX/4yp512Wh566KFcddVV2WCDDbLNNttk5syZGT169Grbzp07NzNmzFi5fPvtt2fWrFl5/etfP2zzwx9DKAHQkV122SXz589PkixfvjxjxozJ4Ycfni233DIf+chHkiRnn312TjvttJx//vmrbbv//vuv3Pbhhx/OTjvtlAMPPHB4nwD8EYQSAM/anDlzMn78+IwdO3a19YsXL04p5Rm3vfzyy3PQQQdl88037+aIsE4IJQCetVmzZmX69Okrl9///vfnS1/6UrbYYovMnTt3rdu+973v7faIsE64mBuAZ2Xp0qWZPXt2jjrqqJXrPvaxj+Wee+7Jm9/85pxzzjnNbe+777789Kc/zWte85rhGBX+ZEIJgGfl6quvztSpU7Pttts+7XdvetObcsUVVzS3veyyy3L44Ydn44037uaIsM4IJQCelUsvvXS102533nnnyp9nz56dXXfdteNtod8JJQA6tmTJklxzzTU54ogjVq475ZRTMnHixEyePDnf/va385nPfCZJMjAwkOOPP37l4+6+++7cc8892W+//YZ9bvhjuZgbgI5tvvnmeeihh1Zb1zrVNm3atFxwwQUrl8eNG5d77723q/PBuuaIEgBAgyNKAOuJcad8o9cjjCh3n3FIr0dgPeCIEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQMOzCqVSygallBd2axgAgH6y1lAqpfzfUsoLSynPT3JrkjtKKf/Q/dEAAHqrkyNKL621Lkry+iT/nmSHJG/t6lQAAH2gk1DauJSycQZD6apa6xNJanfHAgDovU5C6XNJ7k7y/CTXlVLGJlnUzaEAAPrBRmt7QK317CRnr7LqV6WU/bs3EgBAf+jkYu6Thi7mLqWUC0spP05ywDDMBgDQU52cejt26GLuA5NsneSdSc7o6lQAAH2gk1AqQ98PTvLFWustq6wDABixOgmleaWUb2cwlL5VSnlBkhXdHQsAoPfWejF3kuOSTEnyy1rrklLKn2fw9BsAwIjWybveVpRS7kqycyll02GYCQCgL6w1lEopxyc5Kcn2SeYn2TvJD+KdbwDACNfJNUonJfnLJL+qte6fZI8kD3R1KgCAPtBJKD1ea308SUopm9Rab0+yS3fHAgDovU4u5l5QSnlRkiuTXFNKeSTJwu6OBQDQe51czH340I8fLqXMTbJFkm92dSoAgD7QDKVSyp+tYfVPh76PSvJwVyYCAOgTz3REaV6SmtXvwv3kck3yki7OBQDQc81QqrXuOJyDAAD0m7W+662UcngpZYtVll9USnl9d8cCAOi9Tm4PcGqt9bdPLtRaH01yavdGAgDoD52E0poe08ltBQAA1mudhNJAKeVTpZTxpZSXlFLOyuCF3gAAI1onofR3SZYm+WqSy5L8Psm7uzkUAEA/6OSGk4uTnDIMswAA9JVOjigBADwnCSUAgIZnDKVSyoallBnDNQwAQD95xlCqtS5PctgwzQIA0Fc6uR/S90sp52TwXW+Ln1xZa/1x16YCAOgDnYTSK4a+n7bKuprkgHU/DgBA/+jk9gD7D8cgAAD9ppMPxd22lHJhKeXqoeWXllKO6/5oAAC91cntAWYm+VaS0UPLv0jynm4NBADQLzoJpa1qrZclWZEktdZlSZZ3dSoAgD7QSSgtLqX8eQYv4E4pZe8kv+3qVAAAfaCTd729N8nsJONLKd9PsnWSI7s6FQBAH+jkXW8/LqXsl2SXJCXJHbXWJ7o+GQBAj601lEopmyb5X0lemcHTb9eXUs6vtT7e7eEAAHqpk1NvX0ryWJJ/GVqenuTLSY7q1lAAAP2gk1Dapdb6slWW55ZSbunWQAAA/aKTd73dPPROtyRJKWWvJN/v3kgAAP2hkyNKeyV5Wynl10PLOyS5rZTy0yS11jq5a9MBAPRQJ6H02q5PAQDQhzq5PcCvhmMQAIB+08k1SgAAz0lCCQCgYa2hVEo5sZSy5XAMAwDQTzo5ovQXSX5USrmslPLaUkrp9lAAAP1graFUa/1AkglJLkzyjiR3llL+uZQyvsuzAQD0VEfXKNVaa5L/GvpalmTLJJeXUj7exdkAAHqqkw/F/fskb0/yYJILkvxDrfWJUsoGSe5McnJ3RwQA6I1Obji5VZIjnno/pVrrilLKod0ZCwCg9zo59fbvSR5+cqGU8oKhz3tLrfW2bg0GANBrnYTSeUl+t8ry4qF1AAAjWiehVIYu5k4yeMotnZ2yAwBYr3USSr8spfx9KWXjoa+Tkvyy24MBAPRaJ6F0QpJXJLk3yYIkeyV5VzeHAgDoB2s9hVZrvT/JMcMwCwBAX+nkPkqbJjkuye5JNn1yfa312C7OBQDQc52cevtyBj/v7TVJvpdk+ySPdXMoAIB+0Eko7VRr/WCSxbXWi5MckmRSd8cCAOi9TkLpiaHvj5ZSJibZIsm4rk0EANAnOrkf0udLKVsm+UCS2UlGJflgV6cCAOgDzxhKQx98u6jW+kiS65K8ZFimAgDoA8946m3oLtwnDtMsAAB9pZNrlK4ppbyvlPLiUsqfPfnV9ckAAHqsk2uUnrxf0rtXWVfjNBwAMMJ1cmfuHYdjEACAftPJnbnftqb1tdYvrftxAAD6Ryen3v5ylZ83TfLXSX6cRCgBACNaJ6fe/m7V5VLKFhn8WBMAgBGtk3e9PdWSJBPW9SAAAP2mk2uU/l8G3+WWDIbVS5Nc1s2hAAD6QSfXKH1ylZ+XJflVrXVBl+YBAOgbnYTSr5PcV2t9PElKKZuVUsbVWu/u6mQAAD3WyTVKX0uyYpXl5UPrAABGtE5CaaNa69InF4Z+fl73RgIA6A+dhNIDpZS/fXKhlHJYkge7NxIAQH/o5BqlE5J8pZRyztDygiRrvFs3AMBI0skNJ/8zyd6llFFJSq31se6PBQDQe2s99VZK+edSyotqrb+rtT5WStmylPLR4RgOAKCXOrlG6aBa66NPLtRaH0lycPdGAgDoD52E0oallE2eXCilbJZkk2d4PADAiNDJxdyXJJlTSvliBj/K5NgkX+rqVAAAfaCTi7k/Xkr5SZK/SVKSfKTW+q2uTwYA0GOdHFFKrfWbSb6ZJKWUvyqlfLbW+u6uTgYA0GMdhVIpZUqS6UmOTnJXkn/t5lAAAP2gGUqllJ2THJPBQHooyVczeB+l/YdpNgCAnnqmI0q3J7k+yetqrf+RJKWUGcMyFQBAH3im2wO8Icl/JZlbSvlCKeWvM3gxNwDAc0IzlGqt/1ZrPTrJrkm+m2RGkm1LKeeVUg4cpvkAAHpmrTecrLUurrV+pdZ6aJLtk8xPckrXJwMA6LFO7sy9Uq314Vrr52qtB3RrIACAfvGsQgkA4LlEKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoEEoAAA1CCQCgQSgBADQIJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0CCUAgAahBADQIJQAABqEEgBAg1ACAGgQSgAADUIJAKBBKAEANAglAIAGoQQA0CCUAAAahBIAQINQAgBoKLXWdb/TUh5I8qt1vuP111ZJHuz1EKyV16n/eY3WD16n9YPX6b+NrbVuvaZfdCWUWF0pZaDWOq3Xc/DMvE79z2u0fvA6rR+8Tp1x6g0AoEEoAQA0CKXh8fleD0BHvE79z2u0fvA6rR+8Th1wjRIAQIMjSgAADUKpi0opry2l3FFK+Y9Syim9noenK6VcVEq5v5Tys17PQlsp5cWllLmllNtKKT8vpZzU65lYXSll01LKTaWUW4Zeo//T65loK6VsWEq5uZTy9V7P0u+EUpeUUjZM8tkkByV5aZLppZSX9nYq1mBmktf2egjWalmS/11r3S3J3kne7f9PfecPSQ6otb4syZQkry2l7N3jmWg7KcltvR5ifSCUuuflSf6j1vrLWuvSJLOSHNbjmXiKWut1SR7u9Rw8s1rrfbXWHw/9/FgG/8CP6e1UrKoO+t3Q4sZDXy6C7UOllO2THJLkgl7Psj4QSt0zJsk9qywviD/s8CcrpYxLskeSG3s7CU81dDpnfpL7k1xTa/Ua9adPJzk5yYpeD7I+EErdU9awzr+u4E9QShmV5Iok76m1Lur1PKyu1rq81jolyfZJXl5KmdjrmVhdKeXQJPfXWuf1epb1hVDqngVJXrzK8vZJFvZoFljvlVI2zmAkfaXW+q+9noe2WuujSb4b1//1o79K8rellLszeEnIAaWUS3o7Un8TSt3zoyQTSik7llKel+SYJLN7PBOsl0opJcmFSW6rtX6q1/PwdKWUrUspLxr6ebMkf5Pk9t5OxVPVWv+p1rp9rXVcBv+7dG2t9S09HquvCaUuqbUuS3Jikm9l8MLTy2qtP+/tVDxVKeXSJD9IskspZUEp5bhez8Qa/VWSt2bwX7/zh74O7vVQrGa7JHNLKT/J4D8Ur6m1eus56z135gYAaHBECQCgQSgBADQIJQCABqEEANAglAAAGoQSMCxKKX9RSplVSvnPUsqtpZR/L6XsXEr5Wa9nA2jZqNcDACPf0A0j/y3JxbXWY4bWTUmybU8HA1gLR5SA4bB/kidqrec/uaLWOj+rfHB0KWVcKeX6UsqPh75eMbR+u1LKdUM3mfxZKeV/DH346syh5Z+WUmYMPXZ8KeWbpZR5Q/vadWj9UUOPvaWUct3wPnVgfeaIEjAcJiZZ24dw3p/k1bXWx0spE5JcmmRakjcl+Vat9WOllA2TbJ5kSpIxtdaJSfLkR2ck+XySE2qtd5ZS9kpybpIDknwoyWtqrfeu8liAtRJKQL/YOMk5Q6fklifZeWj9j5JcNPShuFfWWueXUn6Z5CWllH9J8o0k3y6ljEryiiRfGzzTlyTZZOj795PMLKVclsQH6gIdc+oNGA4/T7LnWh4zI8lvkrwsg0eSnpcktdbrkuyb5N4kXy6lvK3W+sjQ476b5N1JLsjg37NHa61TVvnabWgfJ5WV5doAAAD+SURBVCT5QJIXJ5lfSvnzdfz8gBFKKAHD4dokm5RS/ueTK0opf5lk7CqP2SLJfbXWFRn8ANwNhx43Nsn9tdYvJLkwydRSylZJNqi1XpHkg0mm1loXJbmrlHLU0HallPKyoZ/H11pvrLV+KMmDGQwmgLUSSkDX1cFP3z48yauHbg/w8yQfTrJwlYedm+TtpZQfZvC02+Kh9a/K4FGgm5O8IclnkoxJ8t1SyvwkM5P809Bj35zkuFLKLRk8inXY0PpPDF30/bMk1yW5pRvPExh5yuDfLwAAnsoRJQCABqEEANAglAAAGoQSAECDUAIAaBBKAAANQgkAoEEoAQA0/H81aFDI3jfBMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.figure(figsize = (10,10)) \n",
        "plt.bar(x,y,align='center') \n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy per class') \n",
        "plt.yticks([]) \n",
        "for i in range(len(y)):\n",
        "    plt.annotate(str(y[i]), xy=(x[i],y[i]), ha='center', va='bottom')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7YwZfoGf0Bn"
      },
      "source": [
        "Allo stesso modo dei modelli precedenti, per effettaure una prova di predizione, carico il *tf-idf vectorizer* e il modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svn3h9KDf0Bn",
        "outputId": "c1b39bdb-a61a-4295-eecc-25e6041c814f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Hi all, Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "with open('tfidf_vectorizer.pk','rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "nn = tf.keras.models.load_model('neural_network.h5')\n",
        "\n",
        "\n",
        "classes = {0 : 'Anger', 1 : 'Fear',2 : 'Joy', 3 : 'Sadness',4 : 'Neutral'}\n",
        "\n",
        "\n",
        "query = \"Hi all\"\n",
        "transformed_query = tfidf_vectorizer.transform([query]).toarray()\n",
        "prediction_class = nn.predict(transformed_query)\n",
        "prediction = np.argmax(prediction_class, axis = 1)\n",
        "print(f\"Query: {query}, Sentiment: {classes[prediction[0]]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Classification.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}